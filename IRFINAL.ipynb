{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IR ALL PRACTICALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a program for pre-processing of a text document such as conversion to lowercase, stop word removal, remove punctuation, removing numbers, stemming, lemmatization, number to text, remove whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yashp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yashp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yashp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is an example text with numbers like 123, punctuation! and stopwords. Running and ran. 456\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is an example text with numbers like 123, punctuation! and stopwords. running and ran. 456'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert text to lowercase\n",
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'text',\n",
       " 'with',\n",
       " 'numbers',\n",
       " 'like',\n",
       " '123',\n",
       " ',',\n",
       " 'punctuation',\n",
       " '!',\n",
       " 'and',\n",
       " 'stopwords',\n",
       " '.',\n",
       " 'running',\n",
       " 'and',\n",
       " 'ran',\n",
       " '.',\n",
       " '456']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "words = nltk.word_tokenize(text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'text',\n",
       " 'with',\n",
       " 'numbers',\n",
       " 'like',\n",
       " '123',\n",
       " 'punctuation',\n",
       " 'and',\n",
       " 'stopwords',\n",
       " 'running',\n",
       " 'and',\n",
       " 'ran',\n",
       " '456']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "words = [word for word in words if word not in string.punctuation]\n",
    "words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'text',\n",
       " 'with',\n",
       " 'numbers',\n",
       " 'like',\n",
       " 'one hundred and twenty-three',\n",
       " 'punctuation',\n",
       " 'and',\n",
       " 'stopwords',\n",
       " 'running',\n",
       " 'and',\n",
       " 'ran',\n",
       " 'four hundred and fifty-six']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove numbers and convert them to text\n",
    "words = [num2words(word) if word.isnumeric() else word for word in words]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example',\n",
       " 'text',\n",
       " 'numbers',\n",
       " 'like',\n",
       " 'one hundred and twenty-three',\n",
       " 'punctuation',\n",
       " 'stopwords',\n",
       " 'running',\n",
       " 'ran',\n",
       " 'four hundred and fifty-six']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [word for word in words if word not in stop_words]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exampl',\n",
       " 'text',\n",
       " 'number',\n",
       " 'like',\n",
       " 'one hundred and twenty-thre',\n",
       " 'punctuat',\n",
       " 'stopword',\n",
       " 'run',\n",
       " 'ran',\n",
       " 'four hundred and fifty-six']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "words = [stemmer.stem(word) for word in words]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exampl',\n",
       " 'text',\n",
       " 'number',\n",
       " 'like',\n",
       " 'one hundred and twenty-thre',\n",
       " 'punctuat',\n",
       " 'stopword',\n",
       " 'run',\n",
       " 'ran',\n",
       " 'four hundred and fifty-six']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [lemmatizer.lemmatize(word) for word in words]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exampl',\n",
       " 'text',\n",
       " 'number',\n",
       " 'like',\n",
       " 'one hundred and twenty-thre',\n",
       " 'punctuat',\n",
       " 'stopword',\n",
       " 'run',\n",
       " 'ran',\n",
       " 'four hundred and fifty-six']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove whitespace\n",
    "words = [word.strip() for word in words if word.strip()]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implement a program for retrieval of documents using inverted\n",
    "files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {\n",
    "    1: \"The quick brown fox jumps over the lazy dog\",\n",
    "    2: \"A brown fox is quick and agile\",\n",
    "    3: \"The lazy dog sleeps all day\",\n",
    "    4: \"The sun is shining brightly\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an inverted index\n",
    "inverted_index = {}\n",
    "for doc_id, text in documents.items():\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in inverted_index:\n",
    "            inverted_index[word] = []\n",
    "        inverted_index[word].append(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve documents containing a keyword\n",
    "def retrieve_documents(keyword):\n",
    "    keyword = keyword.lower()\n",
    "    if keyword in inverted_index:\n",
    "        return [documents[doc_id] for doc_id in inverted_index[keyword]]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"lazy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for documents containing the query terms\n",
    "query_terms = query.split()\n",
    "matching_documents = []\n",
    "for term in query_terms:\n",
    "    matching_documents.extend(retrieve_documents(term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Documents:\n",
      "The quick brown fox jumps over the lazy dog\n",
      "The lazy dog sleeps all day\n"
     ]
    }
   ],
   "source": [
    "# Print the matching documents\n",
    "if matching_documents:\n",
    "    print(\"Matching Documents:\")\n",
    "    for doc in matching_documents:\n",
    "        print(doc)\n",
    "else:\n",
    "    print(\"No matching documents found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement e-mail spam filtering using text classification\n",
    "algorithm with appropriate dataset. Analyse the performance of\n",
    "the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Perform text classification using Naive Bayes/Logistic\n",
    "Regression/ Support Vector Machines /Decision Tree\n",
    "Classifier. Use dataset \"Economic news article tone and\n",
    "relevance\" news articles, which were tagged as relevant or not\n",
    "relevant to the US Economy. Keep the required columns and\n",
    "save it in a dataframe. Explore the process of training and\n",
    "testing text classifier for this dataset and analyse the\n",
    "performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>Yields on CDs Fell in the Latest Week</td>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>The Morning Brief: White House Seeks to Limit ...</td>\n",
       "      <td>The Wall Street Journal Online&lt;/br&gt;&lt;/br&gt;The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no</td>\n",
       "      <td>Banking Bill Negotiators Set Compromise --- Pl...</td>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>Manager's Journal: Sniffing Out Drug Abusers I...</td>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>Currency Trading: Dollar Remains in Tight Rang...</td>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>yes</td>\n",
       "      <td>Sawyer Sees Strong Economy For 2 Years, Truce ...</td>\n",
       "      <td>Secretary of Commerce Charles W. Sawyer said y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>no</td>\n",
       "      <td>Oil's losses are airlines' gains</td>\n",
       "      <td>U.S. stocks inched up last week, overcoming co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>no</td>\n",
       "      <td>Full Senate to vote on Bernanke; PANEL ADVANCE...</td>\n",
       "      <td>Ben S. Bernanke cleared a key hurdle Thursday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>no</td>\n",
       "      <td>Reinventing Opportunities</td>\n",
       "      <td>The White House's push to contract out many fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>no</td>\n",
       "      <td>Stocks Rise On News of Auto Output: Dow Climbs...</td>\n",
       "      <td>NEW YORK. April 17-Automobile stocks put on th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     relevance                                           headline  \\\n",
       "0          yes              Yields on CDs Fell in the Latest Week   \n",
       "1           no  The Morning Brief: White House Seeks to Limit ...   \n",
       "2           no  Banking Bill Negotiators Set Compromise --- Pl...   \n",
       "3           no  Manager's Journal: Sniffing Out Drug Abusers I...   \n",
       "4          yes  Currency Trading: Dollar Remains in Tight Rang...   \n",
       "...        ...                                                ...   \n",
       "7995       yes  Sawyer Sees Strong Economy For 2 Years, Truce ...   \n",
       "7996        no                   Oil's losses are airlines' gains   \n",
       "7997        no  Full Senate to vote on Bernanke; PANEL ADVANCE...   \n",
       "7998        no                          Reinventing Opportunities   \n",
       "7999        no  Stocks Rise On News of Auto Output: Dow Climbs...   \n",
       "\n",
       "                                                   text  \n",
       "0     NEW YORK -- Yields on most certificates of dep...  \n",
       "1     The Wall Street Journal Online</br></br>The Mo...  \n",
       "2     WASHINGTON -- In an effort to achieve banking ...  \n",
       "3     The statistics on the enormous costs of employ...  \n",
       "4     NEW YORK -- Indecision marked the dollar's ton...  \n",
       "...                                                 ...  \n",
       "7995  Secretary of Commerce Charles W. Sawyer said y...  \n",
       "7996  U.S. stocks inched up last week, overcoming co...  \n",
       "7997  Ben S. Bernanke cleared a key hurdle Thursday ...  \n",
       "7998  The White House's push to contract out many fe...  \n",
       "7999  NEW YORK. April 17-Automobile stocks put on th...  \n",
       "\n",
       "[8000 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"REL.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text', 'relevance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yashp\\AppData\\Local\\Temp\\ipykernel_12792\\2245969871.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text'] = data['text'].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # You can add more text preprocessing steps here if needed\n",
    "    return text\n",
    "\n",
    "data['text'] = data['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text']\n",
    "y = data['relevance']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yashp\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\yashp\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\yashp\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "nb_predictions = nb_classifier.predict(X_test_tfidf)\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "nb_classification_report = classification_report(y_test, nb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yashp\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\yashp\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\yashp\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "lr_classifier = LogisticRegression(max_iter=1000)\n",
    "lr_classifier.fit(X_train_tfidf, y_train)\n",
    "lr_predictions = lr_classifier.predict(X_test_tfidf)\n",
    "lr_accuracy = accuracy_score(y_test, lr_predictions)\n",
    "lr_classification_report = classification_report(y_test, lr_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier:\n",
      "Accuracy: 0.814375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.81      1.00      0.90      1303\n",
      "    not sure       0.00      0.00      0.00         1\n",
      "         yes       0.00      0.00      0.00       296\n",
      "\n",
      "    accuracy                           0.81      1600\n",
      "   macro avg       0.27      0.33      0.30      1600\n",
      "weighted avg       0.66      0.81      0.73      1600\n",
      "\n",
      "\n",
      "Logistic Regression Classifier:\n",
      "Accuracy: 0.8125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.83      0.97      0.89      1303\n",
      "    not sure       0.00      0.00      0.00         1\n",
      "         yes       0.48      0.12      0.19       296\n",
      "\n",
      "    accuracy                           0.81      1600\n",
      "   macro avg       0.44      0.36      0.36      1600\n",
      "weighted avg       0.76      0.81      0.76      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes Classifier:\")\n",
    "print(f\"Accuracy: {nb_accuracy}\")\n",
    "print(nb_classification_report)\n",
    "\n",
    "print(\"\\nLogistic Regression Classifier:\")\n",
    "print(f\"Accuracy: {lr_accuracy}\")\n",
    "print(lr_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Implement Page Rank Algorithm. (Use python/beautiful soup\n",
    "for implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the link structure of web pages\n",
    "links = {\n",
    "    'Page A': ['Page B', 'Page C'],\n",
    "    'Page B': ['Page A'],\n",
    "    'Page C': ['Page A', 'Page B'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Page A': 1.0, 'Page B': 1.0, 'Page C': 1.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_ranks = {page: 1.0 for page in links }\n",
    "page_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 20\n",
    "damping_factor = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(iterations):\n",
    "    new_page_ranks = {}\n",
    "    for page in links:\n",
    "        rank = (1 - damping_factor) + damping_factor * sum(\n",
    "            page_ranks[link] / len(links[link]) for link in links if page in links[link]\n",
    "        )\n",
    "        new_page_ranks[page] = rank\n",
    "    page_ranks = new_page_ranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page A: 1.2982\n",
      "Page B: 1.0000\n",
      "Page C: 0.7018\n"
     ]
    }
   ],
   "source": [
    "# Display the final PageRank values\n",
    "for page, rank in page_ranks.items():\n",
    "    print(f'{page}: {rank:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Build the web crawler to pull IMDB information and analyse the\n",
    "ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "url =\"https://www.imdb.com/chart/top/\"\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = requests.get(url, headers= HEADERS)\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "#print(soup.prettify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapped_movies = soup.find_all(class_=\"ipc-title__text\")\n",
    "scrapped_ratings = soup.find_all(class_=\"ipc-rating-star ipc-rating-star--base ipc-rating-star--imdb ratingGroup--imdb-rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings=[]\n",
    "for rating in scrapped_ratings:\n",
    "  rating=rating.get('aria-label',None)\n",
    "  if rating:\n",
    "    ratings.append(rating)\n",
    "#ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge=[]\n",
    "merge.append([scrapped_movies,ratings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Build a web crawler to get web contents like PPT and Image\n",
    "files. The files should be downloaded on the local machine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
